from __future__ import annotations
import argparse, json, hashlib, os
from pathlib import Path

from .io_utils import RUNS_ROOT, ensure_dir, write_json, write_yaml, write_text
from .parser import prompt_to_scenario
from .utils import timestamp_slug, slugify, git_hash_fallback
from . import __version__

# Optional modules (present in your repo; guard just in case)
try:
    from .rollout import run_batch
except Exception:
    run_batch = None  # type: ignore

try:
    from .metrics import aggregate, write_summary, calibrate_from_anchors, write_site_profile, write_calibration_json
except Exception:
    aggregate = None  # type: ignore
    write_summary = None  # type: ignore
    calibrate_from_anchors = None  # type: ignore
    write_site_profile = None  # type: ignore
    write_calibration_json = None  # type: ignore

try:
    from .reporter import write_report
except Exception:
    write_report = None  # type: ignore

try:
    from .coverage import build_coverage   # taxonomy coverage
except Exception:
    build_coverage = None  # type: ignore

try:
    from .report_html import generate_report
except Exception:
    generate_report = None  # type: ignore

try:
    from .schema_validate import validate_scenario  # (ok if missing)
except Exception:
    validate_scenario = None  # type: ignore

from .sim_one import run_one


def _make_run_dir(prompt: str, name: str | None) -> Path:
    stamp = timestamp_slug()
    label = slugify(name or prompt)
    run_dir = RUNS_ROOT / f"{stamp}_{label}"
    ensure_dir(run_dir)
    return run_dir

def _write_core_files(run_dir: Path, prompt: str, n_runs: int, seed_base: int, scn: dict) -> dict:
    seeds = {"base_seed": seed_base, "seeds": [seed_base + i for i in range(n_runs)]}
    manifest = {
        "prompt": prompt,
        "n_runs": n_runs,
        "seed_base": seed_base,
        "version": __version__,
        "git": git_hash_fallback(),
    }
    write_yaml(run_dir / "scenario.yaml", scn)
    write_json(run_dir / "seeds.json", seeds)
    write_json(run_dir / "manifest.json", manifest)

    # Optional: schema validation & context summary
    if validate_scenario is not None:
        try:
            ok, errors, summary = validate_scenario(scn)
            write_json(run_dir / "validation.json", {"ok": bool(ok), "errors": list(errors or []), "context_summary": summary or {}})
        except Exception as e:
            write_json(run_dir / "validation.json", {"ok": False, "errors": [f"validator exception: {type(e).__name__}: {e}"], "context_summary": {}})
    return {"seeds": seeds, "manifest": manifest}

def _sha256_file(path: Path) -> str:
    h = hashlib.sha256(); h.update(path.read_bytes()); return h.hexdigest()

def _digest_csvs(per_run_dir: Path) -> str:
    h = hashlib.sha256()
    if per_run_dir.exists():
        for p in sorted(per_run_dir.rglob("*.csv")):
            h.update(p.read_bytes())
    return h.hexdigest()

def cmd_simulate(args: argparse.Namespace) -> int:
    prompt: str = args.prompt
    n_runs: int = int(args.runs)
    seed_base: int = int(args.seed)
    scn = prompt_to_scenario(prompt, n_runs=n_runs)
    run_dir = _make_run_dir(prompt, args.name)
    _write_core_files(run_dir, prompt, n_runs, seed_base, scn)
    write_text(run_dir / "RUN_README.md", """# EdgeSim Run (V0 Step 1)

This folder was generated by `edgesim simulate`.

**Includes:**
- `scenario.yaml`
- `seeds.json`
- `manifest.json`
- `validation.json` (if validator available)
""")
    status = " (schema validated)" if (run_dir / "validation.json").exists() else ""
    print(f"\n[EdgeSim] Created run @ {run_dir}\n- scenario.yaml\n- seeds.json\n- manifest.json\n- validation.json{status}\n")
    return 0

def cmd_run_batch(args: argparse.Namespace) -> int:
    if run_batch is None:
        raise SystemExit("Batch runner modules not available. Add rollout/metrics/reporter or skip this command.")

    prompt: str = args.prompt
    n_runs: int = int(args.runs)
    seed_base: int = int(args.seed)
    profile: str = args.profile

    # If user specified a site profile, propagate via env so deep code can pick it up
    if getattr(args, "site", None):
        os.environ["EDGESIM_SITE"] = str(args.site)

    # Parse and prep
    scn = prompt_to_scenario(prompt, n_runs=n_runs)
    run_dir = _make_run_dir(prompt, args.name)
    files = _write_core_files(run_dir, prompt, n_runs, seed_base, scn)
    seeds = files["seeds"]["seeds"]

    # Execute batch
    if hasattr(args, "time_budget_min") or hasattr(args, "auto_degrade"):
        try:
            run_batch(prompt, scn, seeds, run_dir, profile=profile,
                      time_budget_min=getattr(args, "time_budget_min", None),
                      auto_degrade=bool(getattr(args, "auto_degrade", False)))
        except TypeError:
            run_batch(prompt, scn, seeds, run_dir, profile=profile)
    else:
        run_batch(prompt, scn, seeds, run_dir, profile=profile)

    # Coverage
    coverage = {}
    if build_coverage is not None:
        try:
            coverage = build_coverage(run_dir / "per_run")
        finally:
            write_json(run_dir / "coverage.json", coverage)

    # Summary
    if aggregate is not None:
        summary = aggregate(run_dir / "per_run")
    else:
        summary = {"runs": 0, "successes": 0, "failures": 0, "avg_time": 0.0, "avg_steps": 0}
    if write_summary is not None:
        write_summary(run_dir, summary)

    # Digests (per_run CSVs + world)
    man_path = run_dir / "manifest.json"
    try:
        man = json.loads(man_path.read_text(encoding="utf-8"))
    except Exception:
        man = {}
    man["per_run_digest"] = _digest_csvs(run_dir / "per_run")
    # choose batch-level world.json if present, else first per-run world
    world_path = run_dir / "world.json"
    if not world_path.exists():
        pr = run_dir / "per_run"
        for sub in sorted(pr.glob("run_*")):
            cand = sub / "world.json"
            if cand.exists():
                world_path = cand
                break
    if world_path.exists():
        man["world_sha256"] = _sha256_file(world_path)
    write_json(man_path, man)

    # reload manifest so reports get new fields
    try:
        files["manifest"] = json.loads(man_path.read_text(encoding="utf-8"))
    except Exception:
        pass

    # Legacy Markdown
    if write_report is not None:
        write_report(run_dir, prompt, files["manifest"], coverage, summary)

    # HTML report
    if generate_report is not None:
        try:
            out_html = generate_report(run_dir)
            print(f"[EdgeSim] HTML report: {out_html}")
        except Exception as e:
            print(f"[EdgeSim][WARN] Failed to generate HTML report: {e}")

    print(f"\n[EdgeSim] Batch done @ {run_dir}\n- per_run/*.csv\n- summary.json\n- coverage.json\n- report.md\n- report.html\n")
    return 0

def cmd_run_one(args: argparse.Namespace) -> int:
    prompt: str = args.prompt
    gui: bool = bool(args.gui)
    realtime: bool = bool(args.realtime)
    dt: float = float(args.dt)
    slowmo: float = float(args.slowmo)

    # Pass site via env if provided
    if getattr(args, "site", None):
        os.environ["EDGESIM_SITE"] = str(args.site)

    scn = prompt_to_scenario(prompt, n_runs=1)
    if getattr(args, "debug_visuals", False):
        scn["debug_visuals"] = True
    run_dir = _make_run_dir(prompt, args.name)
    _write_core_files(run_dir, prompt, 1, args.seed, scn)
    out = run_one(prompt, scn, run_dir, dt=dt, realtime=realtime, gui=gui, sleep_scale=slowmo)

    # Digests for single run (csv + world)
    man_path = run_dir / "manifest.json"
    try:
        man = json.loads(man_path.read_text(encoding="utf-8"))
    except Exception:
        man = {}
    man["per_run_digest"] = _digest_csvs(run_dir)
    world_path = run_dir / "world.json"
    if world_path.exists():
        man["world_sha256"] = _sha256_file(world_path)
    write_json(man_path, man)

    print(f"\n[EdgeSim] Single rollout @ {run_dir}\n- success={out['success']} time={out['time']:.2f}s steps={out['steps']}\n- run_one.csv\n")
    return 0

# ---- NEW: verify command (digest checks) ----
def cmd_verify(args: argparse.Namespace) -> int:
    batch_dir = Path(args.batch).resolve()
    man_path = batch_dir / "manifest.json"
    if not man_path.exists():
        raise SystemExit(f"manifest.json not found in {batch_dir}")
    try:
        man = json.loads(man_path.read_text(encoding="utf-8"))
    except Exception as e:
        raise SystemExit(f"manifest.json unreadable: {e}")

    expect_csv = man.get("per_run_digest", "NA")
    got_csv = _digest_csvs(batch_dir / "per_run") if (batch_dir / "per_run").exists() else "NA"
    ok_csv = (expect_csv != "NA") and (expect_csv == got_csv)

    expect_world = man.get("world_sha256", "NA")
    got_world = "NA"
    world_path = batch_dir / "world.json"
    
    if not world_path.exists():
        # fallback: grab the first per-run world.json if present
        pr = batch_dir / "per_run"
        for sub in sorted(pr.glob("run_*")):
            cand = sub / "world.json"
            if cand.exists():
                world_path = cand
                break

    if world_path.exists():
        got_world = hashlib.sha256(world_path.read_bytes()).hexdigest()
    ok_world = (expect_world != "NA") and (expect_world == got_world)

    print("[edgesim verify]")
    print(f"- CSV digest: expected={expect_csv[:12]}…  got={got_csv[:12]}…   -> {'PASS' if ok_csv else 'FAIL'}")
    if expect_world == "NA" and not world_path.exists():
        print(f"- world.json: expected=NA  got=NA -> SKIP")
    else:
        print(f"- world.json: expected={expect_world[:12]}…  got={got_world[:12]}… -> {'PASS' if ok_world else 'FAIL'}")

    return 0 if (ok_csv and (ok_world or expect_world=='NA')) else 1

# ---- NEW: calibrate command (anchors -> metrics -> site profile + optional batch calibration.json)
def cmd_calibrate(args: argparse.Namespace) -> int:
    if calibrate_from_anchors is None:
        raise SystemExit("Calibration module not available.")

    anchors_path = Path(args.anchors).resolve()
    if not anchors_path.exists():
        raise SystemExit(f"anchors CSV not found: {anchors_path}")

    site: str = args.site
    site_profiles = (RUNS_ROOT.parent / "site_profiles").resolve()
    ensure_dir(site_profiles)

    metrics = calibrate_from_anchors(anchors_path)
    # write site profile (with tuned defaults derived from metrics)
    site_path = write_site_profile(site, metrics, anchors_path=anchors_path) if write_site_profile else (site_profiles / f"{site}.json")

    # optionally mirror into a batch dir so the report can render it
    batch_dir: Path | None = None
    if args.run is not None:
        batch_dir = Path(args.run).resolve()
        if not batch_dir.exists():
            raise SystemExit(f"--run path not found: {batch_dir}")
        if write_calibration_json:
            write_calibration_json(batch_dir, site, anchors_path, metrics)
        else:
            write_json(batch_dir / "calibration.json", {"site": site, "anchors": str(anchors_path), "metrics": metrics})

    print(f"[EdgeSim] Calibrated site '{site}':")
    print(json.dumps(metrics, indent=2))
    print(f"- wrote {site_path}")
    if batch_dir is not None:
        print(f"- wrote {batch_dir / 'calibration.json'} (report will display metrics)")

    return 0

def cmd_assert_stress(args: argparse.Namespace) -> int:
    """
    Gate an edge-case batch by coverage/outcome targets.
    Returns 0 on pass, 2 on failure (so CI can fail the job).
    """
    batch = Path(args.run).resolve()
    cov_path = batch / "coverage.json"
    sum_path = batch / "summary.json"

    if not cov_path.exists():
        raise SystemExit(f"coverage.json not found in {batch}")
    if not sum_path.exists():
        raise SystemExit(f"summary.json not found in {batch}")

    try:
        cov = json.loads(cov_path.read_text(encoding="utf-8"))
    except Exception as e:
        raise SystemExit(f"coverage.json unreadable: {e}")
    try:
        summ = json.loads(sum_path.read_text(encoding="utf-8"))
    except Exception as e:
        raise SystemExit(f"summary.json unreadable: {e}")

    # Helpers to safely extract % buckets across possible coverage schemas
    def _pct(container: dict, *keys: str, default: float = 0.0) -> float:
        d = container
        for k in keys:
            if not isinstance(d, dict): return default
            d = d.get(k, {})
        if isinstance(d, (int, float)): return float(d)
        return default

    # Common shapes (your build_coverage likely uses these names)
    human_fallen = _pct(cov, "human_phase_pct", "fallen", default=_pct(cov, "human_phase", "fallen", default=0.0))
    wet_enc     = _pct(cov, "traction_pct", "wet_encountered", default=_pct(cov, "traction", "wet_encountered", default=0.0))
    tight_pct   = _pct(cov, "clearance_bands_pct", "<0.20", default=_pct(cov, "clearance_bands", "<0.20", default=0.0))
    coll_pct    = _pct(cov, "outcomes_pct", "collision_human", default=_pct(cov, "outcomes", "collision_human", default=0.0))
    succ_pct    = _pct(cov, "outcomes_pct", "success", default=_pct(cov, "outcomes", "success", default=0.0))

    # Allow overrides via CLI
    fallen_min   = float(args.fallen_min)
    wet_min      = float(args.wet_min)
    tight_min    = float(args.tight_min)
    collision_min= float(args.collision_min)
    success_max  = float(args.success_max)

    ok = True
    checks = []

    def check(cond: bool, label: str, got: float, target: str):
        nonlocal ok
        ok = ok and cond
        checks.append((cond, f"{label}: {got:.1f}% {target}"))

    check(human_fallen >= fallen_min,     "fallen",          human_fallen,  f"≥ {fallen_min:.1f}%")
    check(wet_enc      >= wet_min,        "wet_encountered", wet_enc,       f"≥ {wet_min:.1f}%")
    check(tight_pct    >= tight_min,      "clearance<0.20",  tight_pct,     f"≥ {tight_min:.1f}%")
    check(coll_pct     >= collision_min,  "collision_human", coll_pct,      f"≥ {collision_min:.1f}%")
    check(succ_pct     <= success_max,    "success",         succ_pct,      f"≤ {success_max:.1f}%")

    print("[edgesim assert-stress]")
    print(f"- runs: {summ.get('runs','?')}, successes: {summ.get('successes','?')}, failures: {summ.get('failures','?')}")
    for cond, msg in checks:
        print(f"- {'PASS' if cond else 'FAIL'}: {msg}")

    return 0 if ok else 2


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="edgesim", description="EdgeSim V0 CLI")
    sub = p.add_subparsers(dest="command", required=True)

    sp = sub.add_parser("simulate", help="Parse prompt → Scenario Graph & create run folder")
    sp.add_argument("prompt", type=str)
    sp.add_argument("--runs", type=int, default=100)
    sp.add_argument("--seed", type=int, default=42)
    sp.add_argument("--name", type=str, default=None)
    sp.set_defaults(func=cmd_simulate)

    sb = sub.add_parser("run-batch", help="Seeded batch → CSVs + summary + report")
    sb.add_argument("prompt", type=str)
    sb.add_argument("--runs", type=int, default=100)
    sb.add_argument("--seed", type=int, default=42)
    sb.add_argument("--name", type=str, default=None)
    sb.add_argument("--profile", type=str, default="minimal", choices=["minimal", "robot", "full"])
    sb.add_argument("--site", type=str, default=None, help="Site profile slug (loads site_profiles/<site>.json via EDGESIM_SITE)")
    sb.add_argument("--time-budget-min", type=float, default=None,
                    help="Soft wall-clock budget (minutes). If ETA exceeds and --auto-degrade is set, stop early.")
    sb.add_argument("--auto-degrade", action="store_true",
                    help="Stop early when the budget would be exceeded.")
    sb.set_defaults(func=cmd_run_batch)

    so = sub.add_parser("run-one", help="Run a single PyBullet rollout (V0)")
    so.add_argument("prompt", type=str)
    so.add_argument("--seed", type=int, default=42)
    so.add_argument("--name", type=str, default=None)
    so.add_argument("--gui", action="store_true", help="Open PyBullet GUI")
    so.add_argument("--realtime", action="store_true", help="Sleep to realtime")
    so.add_argument("--dt", type=float, default=0.05, help="Integrator timestep (seconds)")
    so.add_argument("--slowmo", type=float, default=1.0, help="Slow-motion multiplier for realtime sleep (requires --realtime)")
    so.add_argument("--site", type=str, default=None, help="Site profile slug (loads site_profiles/<site>.json via EDGESIM_SITE)")
    so.add_argument("--debug-visuals", action="store_true", help="Draw debug overlays for aisles/paths (GUI only)")
    so.set_defaults(func=cmd_run_one)

    sv = sub.add_parser("verify", help="Verify reproducibility digests for a finished batch")
    sv.add_argument("batch", type=str, help="Path to runs/<batch_dir>")
    sv.set_defaults(func=cmd_verify)

    # NEW: calibrate
    sc = sub.add_parser("calibrate", help="Compute calibration metrics from anchors CSV and write site profile")
    sc.add_argument("--anchors", required=True, help="Path to anchors.csv")
    sc.add_argument("--site", required=True, help="Site slug (filename for site_profiles/<site>.json)")
    sc.add_argument("--run", default=None, help="Optional runs/<batch_dir> to also write calibration.json for the report")
    sc.set_defaults(func=cmd_calibrate)

    ss = sub.add_parser("assert-stress", help="Assert an edge-case batch hits target coverage/outcomes")
    ss.add_argument("--run", required=True, help="Path to runs/<batch_dir>")
    ss.add_argument("--fallen-min", type=float, default=50.0, help="Min %% of runs with human phase=fallen")
    ss.add_argument("--wet-min", type=float, default=70.0, help="Min %% of runs that encountered wet traction")
    ss.add_argument("--tight-min", type=float, default=40.0, help="Min %% with clearance < 0.20 m")
    ss.add_argument("--collision-min", type=float, default=20.0, help="Min %% with collision_human outcome")
    ss.add_argument("--success-max", type=float, default=60.0, help="Max %% success allowed")
    ss.set_defaults(func=cmd_assert_stress)

    return p

def main() -> int:
    parser = build_parser()
    args = parser.parse_args()
    return args.func(args)

if __name__ == "__main__":
    exit(main())
